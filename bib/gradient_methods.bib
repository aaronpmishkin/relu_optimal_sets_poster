### SGD ###

@Article{robbins1951sgd,
  author       = "Herbert Robbins and Sutton Monro",
  journal      = "Ann. Math. Statist.",
  month        = "09",
  number       = "3",
  pages        = "400--407",
  publisher    = "The Institute of Mathematical Statistics",
  title        = "A Stochastic Approximation Method",
  volume       = "22",
  year         = "1951",
}

@InCollection{bengio2012practical,
  author       = "Yoshua Bengio",
  editor       = "Gr{\'{e}}goire Montavon and Genevieve B. Orr and
                 Klaus{-}Robert M{\"{u}}ller",
  title        = "Practical Recommendations for Gradient-Based Training
                 of Deep Architectures",
  booktitle    = "Neural Networks: Tricks of the Trade - Second
                 Edition",
  series       = "Lecture Notes in Computer Science",
  volume       = "7700",
  pages        = "437--478",
  publisher    = "Springer",
  year         = "2012",
}

%% SGD vs Variance-Reduction
@InProceedings{defazio2019effectiveness,
  author       = "Aaron Defazio and L{\'{e}}on Bottou",
  editor       = "Hanna M. Wallach and Hugo Larochelle and Alina
                 Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily
                 B. Fox and Roman Garnett",
  title        = "On the Ineffectiveness of Variance Reduced
                 Optimization for Deep Learning",
  booktitle    = "Advances in Neural Information Processing Systems 32:
                 {NeurIPS} 2019",
  pages        = "1753--1763",
  year         = "2019",
}

@Article{choi2019empirical,
  title        = "On empirical comparisons of optimizers for deep
                 learning",
  author       = "Dami Choi and Christopher J Shallue and Zachary Nado
                 and Jaehoon Lee and Chris J Maddison and George E
                 Dahl",
  journal      = "arXiv preprint arXiv:1910.05446",
  year         = "2019",
}

# Generalization Properties of SGD #

@InProceedings{zhang2017understanding,
  author       = "Chiyuan Zhang and Samy Bengio and Moritz Hardt and
                 Benjamin Recht and Oriol Vinyals",
  title        = "Understanding deep learning requires rethinking
                 generalization",
  booktitle    = "5th International Conference on Learning
                 Representations, {ICLR} 2017",
  publisher    = "OpenReview.net",
  year         = "2017",
}

% SGD for non-parametric methods
@InProceedings{belkin2019datainterp,
  author       = "Mikhail Belkin and Alexander Rakhlin and Alexandre B.
                 Tsybakov",
  editor       = "Kamalika Chaudhuri and Masashi Sugiyama",
  title        = "Does data interpolation contradict statistical
                 optimality?",
  booktitle    = "The 22nd International Conference on Artificial
                 Intelligence and Statistics, {AISTATS} 2019",
  series       = "Proceedings of Machine Learning Research",
  volume       = "89",
  pages        = "1611--1619",
  publisher    = "{PMLR}",
  year         = "2019",
}

@Article{liang2018just,
  title        = "Just interpolate: {K}ernel ``ridgeless'' regression
                 can generalize",
  author       = "Tengyuan Liang and Alexander Rakhlin",
  journal      = "arXiv preprint arXiv:1808.00387",
  year         = "2018",
}

@Article{belkin2019reconciling,
  title        = "Reconciling modern machine-learning practice and the
                 classical bias--variance trade-off",
  author       = "Mikhail Belkin and Daniel Hsu and Siyuan Ma and Soumik
                 Mandal",
  journal      = "Proceedings of the National Academy of Sciences",
  volume       = "116",
  number       = "32",
  pages        = "15849--15854",
  year         = "2019",
  publisher    = "National Acad Sciences",
}

@InProceedings{schapire1997boosting,
  author       = "Robert E. Schapire and Yoav Freund and Peter Barlett
                 and Wee Sun Lee",
  editor       = "Douglas H. Fisher",
  title        = "Boosting the margin: {A} new explanation for the
                 effectiveness of voting methods",
  booktitle    = "Proceedings of the Fourteenth International Conference
                 on Machine Learning, ({ICML} 1997)",
  pages        = "322--330",
  publisher    = "Morgan Kaufmann",
  year         = "1997",
}

## Hybrid deterministic-stochastic methods ##
@Article{friedlander2012hybrid,
  author       = "Michael P. Friedlander and Mark Schmidt",
  title        = "Hybrid Deterministic-Stochastic Methods for Data
                 Fitting",
  journal      = "{SIAM} J. Scientific Computing",
  volume       = "34",
  number       = "3",
  year         = "2012",
}

@Article{byrd2012sample,
  author       = "Richard H. Byrd and Gillian M. Chin and Jorge Nocedal
                 and Yuchen Wu",
  title        = "Sample size selection in optimization methods for
                 machine learning",
  journal      = "Math. Program.",
  volume       = "134",
  number       = "1",
  pages        = "127--155",
  year         = "2012",
}

@Article{de2016big,
  title        = "Big batch {SGD}: Automated inference using adaptive
                 batch sizes",
  author       = "Soham De and Abhay Yadav and David Jacobs and Tom
                 Goldstein",
  journal      = "arXiv preprint arXiv:1610.05792",
  year         = "2016",
}

%% SGD for large-scale learning
@InProceedings{bottou2007large,
  author       = "L{\'{e}}on Bottou and Olivier Bousquet",
  editor       = "John C. Platt and Daphne Koller and Yoram Singer and
                 Sam T. Roweis",
  title        = "The Tradeoffs of Large Scale Learning",
  booktitle    = "Advances in Neural Information Processing Systems 20:
                 {NeurIPS} 2007",
  pages        = "161--168",
  publisher    = "Curran Associates, Inc.",
  year         = "2007",
}

%% SGD for over-parameterized models
@InProceedings{arora2018overparameterization,
  author       = "Sanjeev Arora and Nadav Cohen and Elad Hazan",
  editor       = "Jennifer G. Dy and Andreas Krause",
  title        = "On the Optimization of Deep Networks: Implicit
                 Acceleration by Overparameterization",
  booktitle    = "Proceedings of the 35th International Conference on
                 Machine Learning, {ICML} 2018",
  series       = "Proceedings of Machine Learning Research",
  volume       = "80",
  pages        = "244--253",
  publisher    = "{PMLR}",
  year         = "2018",
}

@InProceedings{zhou2019analysis,
  author       = "Difan Zou and Quanquan Gu",
  editor       = "Hanna M. Wallach and Hugo Larochelle and Alina
                 Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily
                 B. Fox and Roman Garnett",
  title        = "An Improved Analysis of Training Over-parameterized
                 Deep Neural Networks",
  booktitle    = "Advances in Neural Information Processing Systems 32:
                 {NeurIPS} 2019",
  pages        = "2053--2062",
  year         = "2019",
}

### Almost Sure Convergence of SGD ###

# Weakest Assumptions (according to Orabona)
@Article{bertsekas2000gradient,
  title        = "Gradient convergence in gradient methods with errors",
  author       = "Dimitri P. Bertsekas and John N. Tsitsiklis",
  journal      = "SIAM Journal on Optimization",
  volume       = "10",
  number       = "3",
  pages        = "627--642",
  year         = "2000",
  publisher    = "SIAM",
}

# survey of almost sure convergence of stochastic gradient, subgradient, and proximal methods for finite-sum functions.
@Article{bertsekas2011incremental,
  title        = "Incremental gradient, subgradient, and proximal
                 methods for convex optimization: A survey",
  author       = "Dimitri P Bertsekas",
  journal      = "Optimization for Machine Learning",
  volume       = "2010",
  number       = "1-38",
  pages        = "3",
  year         = "2011",
  publisher    = "MIT press",
}

@Article{lei2019stochastic,
  title        = "Stochastic gradient descent for nonconvex learning
                 without bounded gradient assumptions",
  author       = "Yunwen Lei and Ting Hu and Guiying Li and Ke Tang",
  journal      = "IEEE Transactions on Neural Networks and Learning
                 Systems",
  year         = "2019",
  publisher    = "IEEE",
}

# almost sure convergence of SGD with diminishing step size; larger bound on step-size than previous work.
@InProceedings{nguyen2018sgd,
  author       = "Lam M. Nguyen and Phuong Ha Nguyen and Marten van Dijk
                 and Peter Richt{\'{a}}rik and Katya Scheinberg and
                 Martin Tak{\'{a}}c",
  editor       = "Jennifer G. Dy and Andreas Krause",
  title        = "{SGD} and {H}ogwild! Convergence Without the Bounded
                 Gradients Assumption",
  booktitle    = "Proceedings of the 35th International Conference on
                 Machine Learning, {ICML} 2018",
  series       = "Proceedings of Machine Learning Research",
  volume       = "80",
  pages        = "3747--3755",
  publisher    = "{PMLR}",
  year         = "2018",
}

@PhdThesis{bottou1991approche,
  title        = "Une approche théorique de l'apprentissage
                 connexionniste et applications à la reconnaissance de
                 la parole",
  author       = "L{\'e}on Bottou",
  year         = "1991",
}

### Variance Reduction ###

%% Original SAG Paper
@InProceedings{leroux2012sag,
  author       = "Nicolas {Le Roux} and Mark Schmidt and Francis R.
                 Bach",
  editor       = "Peter L. Bartlett and Fernando C. N. Pereira and
                 Christopher J. C. Burges and L{\'{e}}on Bottou and
                 Kilian Q. Weinberger",
  title        = "A Stochastic Gradient Method with an Exponential
                 Convergence Rate for Finite Training Sets",
  booktitle    = "Advances in Neural Information Processing Systems 25:
                 {NeurIPS} 2012",
  pages        = "2672--2680",
  year         = "2012",
}

%% SAG Journal Version
@Article{schmidt2017sag,
  author       = "Mark Schmidt and Nicolas {Le Roux} and Francis R.
                 Bach",
  title        = "Minimizing finite sums with the stochastic average
                 gradient",
  journal      = "Math. Program.",
  volume       = "162",
  number       = "1-2",
  pages        = "83--112",
  year         = "2017",
}

%% Original SAGA Paper
@InProceedings{defazio2014saga,
  author       = "Aaron Defazio and Francis R. Bach and Simon
                 Lacoste{-}Julien",
  editor       = "Zoubin Ghahramani and Max Welling and Corinna Cortes
                 and Neil D. Lawrence and Kilian Q. Weinberger",
  title        = "{SAGA:} {A} Fast Incremental Gradient Method With
                 Support for Non-Strongly Convex Composite Objectives",
  booktitle    = "Advances in Neural Information Processing Systems 27:
                 {NeurIPS} 2014",
  pages        = "1646--1654",
  year         = "2014",
}

%% Original SVRG Paper
@InProceedings{johnson2013svrg,
  author       = "Rie Johnson and Tong Zhang",
  editor       = "Christopher J. C. Burges and L{\'{e}}on Bottou and
                 Zoubin Ghahramani and Kilian Q. Weinberger",
  title        = "Accelerating Stochastic Gradient Descent using
                 Predictive Variance Reduction",
  booktitle    = "Advances in Neural Information Processing Systems 26:
                 {NeurIPS} 2013",
  pages        = "315--323",
  year         = "2013",
}

### Proximal-Gradient Methods ###

@Article{wright2009sparse,
  author       = "Stephen J. Wright and Robert D. Nowak and M{\'{a}}rio
                 A. T. Figueiredo",
  title        = "Sparse reconstruction by separable approximation",
  journal      = "{IEEE} Trans. Signal Process.",
  volume       = "57",
  number       = "7",
  pages        = "2479--2493",
  year         = "2009",
}

@Article{duchi2009forwardbackwardsplitting,
  author       = "John C. Duchi and Yoram Singer",
  title        = "Efficient Online and Batch Learning Using Forward
                 Backward Splitting",
  journal      = "J. Mach. Learn. Res.",
  volume       = "10",
  pages        = "2899--2934",
  year         = "2009",
}

@Article{parikh2014proximalsurvery,
  author       = "Neal Parikh and Stephen P. Boyd",
  title        = "Proximal Algorithms",
  journal      = "Found. Trends Optim.",
  volume       = "1",
  number       = "3",
  pages        = "127--239",
  year         = "2014",
}

@Article{ghadimi2012optimal1,
  author       = "Saeed Ghadimi and Guanghui Lan",
  title        = "Optimal Stochastic Approximation Algorithms for
                 Strongly Convex Stochastic Composite Optimization {I:}
                 {A} Generic Algorithmic Framework",
  journal      = "{SIAM} J. Optim.",
  volume       = "22",
  number       = "4",
  pages        = "1469--1492",
  year         = "2012",
}

@Article{ghadimi2013optimal2,
  author       = "Saeed Ghadimi and Guanghui Lan",
  title        = "Optimal Stochastic Approximation Algorithms for
                 Strongly Convex Stochastic Composite Optimization,
                 {II:} {S}hrinking Procedures and Optimal Algorithms",
  journal      = "{SIAM} J. Optim.",
  volume       = "23",
  number       = "4",
  pages        = "2061--2089",
  year         = "2013",
}

# Accelerated Proximal-Gradient #
@Article{beck2009fista,
  author       = "Amir Beck and Marc Teboulle",
  title        = "A Fast Iterative Shrinkage-Thresholding Algorithm for
                 Linear Inverse Problems",
  journal      = "{SIAM} J. Imaging Sci.",
  volume       = "2",
  number       = "1",
  pages        = "183--202",
  year         = "2009",
}

@Article{nesterov2007proximalgradient,
  author       = "Yurii E. Nesterov",
  title        = "Gradient methods for minimizing composite functions",
  journal      = "Math. Program.",
  volume       = "140",
  number       = "1",
  pages        = "125--161",
  year         = "2013",
}

@InProceedings{schmidt2011convergence,
  author       = "Mark Schmidt and Nicolas {Le Roux} and Francis R.
                 Bach",
  editor       = "John Shawe{-}Taylor and Richard S. Zemel and Peter L.
                 Bartlett and Fernando C. N. Pereira and Kilian Q.
                 Weinberger",
  title        = "Convergence Rates of Inexact Proximal-Gradient Methods
                 for Convex Optimization",
  booktitle    = "Advances in Neural Information Processing Systems 24:
                 {NeurIPS} 2011",
  pages        = "1458--1466",
  year         = "2011",
}

### "Adaptive" Methods ###

%% Original Adagrad Paper
@Article{duchi2011adagrad,
  author       = "John C. Duchi and Elad Hazan and Yoram Singer",
  title        = "Adaptive Subgradient Methods for Online Learning and
                 Stochastic Optimization",
  journal      = "J. Mach. Learn. Res.",
  volume       = "12",
  pages        = "2121--2159",
  year         = "2011",
}

%% Original Adam Paper
@InProceedings{kingma2015adam,
  author       = "Diederik P. Kingma and Jimmy Ba",
  editor       = "Yoshua Bengio and Yann LeCun",
  title        = "Adam: {A} Method for Stochastic Optimization",
  booktitle    = "3rd International Conference on Learning
                 Representations, {ICLR} 2015",
  year         = "2015",
}

%% AdaDelta
@Article{zeiler2012adadelta,
  title        = "{AdaDelta}: an adaptive learning rate method",
  author       = "Matthew D Zeiler",
  journal      = "arXiv preprint arXiv:1212.5701",
  year         = "2012",
}

%% RMSProp
@Article{tieleman2012rmsprop,
  title        = "Lecture 6.5-{RMSProp}: {D}ivide the gradient by a
                 running average of its recent magnitude",
  author       = "Tijmen Tieleman and Geoffrey Hinton",
  journal      = "Coursera: Neural networks for machine learning",
  year         = "2012",
}

%% AdaBound
@InProceedings{luo2019adabound,
  author       = "Liangchen Luo and Yuanhao Xiong and Yan Liu and Xu
                 Sun",
  title        = "Adaptive Gradient Methods with Dynamic Bound of
                 Learning Rate",
  booktitle    = "7th International Conference on Learning
                 Representations, {ICLR} 2019",
  publisher    = "OpenReview.net",
  year         = "2019",
}

### Acceleration ###

# Deterministic Acceleration #

# original accelerated method
@InProceedings{nesterov1983method,
  title        = "A method for unconstrained convex minimization problem
                 with the rate of convergence ${O}(1/k^{2})$",
  author       = "Yurii Nesterov",
  booktitle    = "Doklady an {USSR}",
  volume       = "269",
  pages        = "543--547",
  year         = "1983",
}

@Article{nemirovsky1985optimal,
  title        = "Optimal methods of smooth convex minimization",
  author       = "Arkadi S Nemirovsky and Yu E Nesterov",
  journal      = "USSR Computational Mathematics and Mathematical
                 Physics",
  volume       = "25",
  number       = "2",
  pages        = "21--30",
  year         = "1985",
  publisher    = "Elsevier",
}

@Article{nesterov1988approach,
  title        = "On an approach to the construction of optimal methods
                 of minimization of smooth convex functions",
  author       = "Yurii Nesterov",
  journal      = "Ekonomika i Mateaticheskie Metody",
  volume       = "24",
  number       = "3",
  pages        = "509--517",
  year         = "1988",
}

# linear coupling
@InProceedings{allen2014linear,
  author       = "Zeyuan {Allen Zhu} and Lorenzo Orecchia",
  editor       = "Christos H. Papadimitriou",
  title        = "Linear {C}oupling: An Ultimate Unification of Gradient
                 and Mirror Descent",
  booktitle    = "8th Innovations in Theoretical Computer Science
                 Conference, {ITCS} 2017",
  series       = "LIPIcs",
  volume       = "67",
  pages        = "3:1--3:22",
  publisher    = "Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r
                 Informatik",
  year         = "2017",
}

# Stochastic Acceleration #


@InProceedings{honorio2012biased,
  author       = "Jean Honorio",
  title        = "Convergence Rates of Biased Stochastic Optimization
                 for Learning Sparse {I}sing Models",
  booktitle    = "Proceedings of the 29th International Conference on
                 Machine Learning, {ICML} 2012",
  publisher    = "icml.cc / Omnipress",
  year         = "2012",
}

@Article{allen-zhu2017katyusha,
  author       = "Zeyuan Allen{-}Zhu",
  title        = "Katyusha: The First Direct Acceleration of Stochastic
                 Gradient Methods",
  journal      = "J. Mach. Learn. Res.",
  volume       = "18",
  pages        = "221:1--221:51",
  year         = "2017",
  URL          = "http://jmlr.org/papers/v18/16-410.html",
}

@InProceedings{allen-zhou2018katyushax,
  author       = "Zeyuan Allen{-}Zhu",
  editor       = "Jennifer G. Dy and Andreas Krause",
  title        = "Katyusha {X}: Practical Momentum Method for Stochastic
                 Sum-of-Nonconvex Optimization",
  booktitle    = "Proceedings of the 35th International Conference on
                 Machine Learning, {ICML} 2018",
  series       = "Proceedings of Machine Learning Research",
  volume       = "80",
  pages        = "179--185",
  publisher    = "{PMLR}",
  year         = "2018",
}

@InProceedings{tang2018restkatyusha,
  author       = "Junqi Tang and Mohammad Golbabaee and Francis Bach and
                 Mike E. Davies",
  editor       = "Samy Bengio and Hanna M. Wallach and Hugo Larochelle
                 and Kristen Grauman and Nicol{\`{o}} Cesa{-}Bianchi and
                 Roman Garnett",
  title        = "Rest-{K}atyusha: {E}xploiting the Solution's Structure
                 via Scheduled Restart Schemes",
  booktitle    = "Advances in Neural Information Processing Systems 31:
                 {NeurIPS} 2018",
  pages        = "427--438",
  year         = "2018",
}

%% accelerated SVRG:
@InProceedings{shang2018asvrg,
  author       = "Fanhua Shang and Licheng Jiao and Kaiwen Zhou and
                 James Cheng and Yan Ren and Yufei Jin",
  editor       = "Jun Zhu and Ichiro Takeuchi",
  title        = "{ASVRG:} {A}ccelerated Proximal {SVRG}",
  booktitle    = "Proceedings of The 10th Asian Conference on Machine
                 Learning, {ACML} 2018",
  series       = "Proceedings of Machine Learning Research",
  volume       = "95",
  pages        = "815--830",
  publisher    = "{PMLR}",
  year         = "2018",
}

%% accelerated SAGA:
@InProceedings{defazio2016pointSaga,
  author       = "Aaron Defazio",
  editor       = "Daniel D. Lee and Masashi Sugiyama and Ulrike von
                 Luxburg and Isabelle Guyon and Roman Garnett",
  title        = "A Simple Practical Accelerated Method for Finite
                 Sums",
  booktitle    = "Advances in Neural Information Processing Systems 29:
                 {NeurIPS} 2016",
  pages        = "676--684",
  year         = "2016",
}

%% loopless SVRG and Katyusha.
@InProceedings{kovalev2020loopless,
  author       = "Dmitry Kovalev and Samuel Horv{\'{a}}th and Peter
                 Richt{\'{a}}rik",
  editor       = "Aryeh Kontorovich and Gergely Neu",
  title        = "Don't Jump Through Hoops and Remove Those Loops:
                 {SVRG} and {K}atyusha are Better Without the Outer
                 Loop",
  booktitle    = "Algorithmic Learning Theory, {ALT} 2020",
  series       = "Proceedings of Machine Learning Research",
  volume       = "117",
  pages        = "451--467",
  publisher    = "{PMLR}",
  year         = "2020",
}

% acceleration with errors
@Article{aspremont2008approximate,
  author       = "Alexandre d'Aspremont",
  title        = "Smooth Optimization with Approximate Gradient",
  journal      = "{SIAM} J. Optim.",
  volume       = "19",
  number       = "3",
  pages        = "1171--1183",
  year         = "2008",
}

@Article{devolder2014first,
  title        = "First-order methods of smooth convex optimization with
                 inexact oracle",
  author       = "Olivier Devolder and Fran{\c{c}}ois Glineur and Yurii
                 Nesterov",
  journal      = "Mathematical Programming",
  volume       = "146",
  number       = "1-2",
  pages        = "37--75",
  year         = "2014",
  publisher    = "Springer",
}

%% acceleration with additive noise
@InProceedings{cohen2018acceleration,
  author       = "Michael Cohen and Jelena Diakonikolas and Lorenzo
                 Orecchia",
  editor       = "Jennifer G. Dy and Andreas Krause",
  title        = "On Acceleration with Noise-Corrupted Gradients",
  booktitle    = "Proceedings of the 35th International Conference on
                 Machine Learning, {ICML} 2018",
  series       = "Proceedings of Machine Learning Research",
  volume       = "80",
  pages        = "1018--1027",
  publisher    = "{PMLR}",
  year         = "2018",
}

%% catalyst: generic acceleration via proximal-point.
% original version.
@InProceedings{lin2015catalyst,
  author       = "Hongzhou Lin and Julien Mairal and Za{\"{\i}}d
                 Harchaoui",
  editor       = "Corinna Cortes and Neil D. Lawrence and Daniel D. Lee
                 and Masashi Sugiyama and Roman Garnett",
  title        = "A Universal Catalyst for First-Order Optimization",
  booktitle    = "Advances in Neural Information Processing Systems 28:
                 {NeurIPS} 2015",
  pages        = "3384--3392",
  year         = "2015",
}

% journal version
@Article{lin2017catalyst,
  author       = "Hongzhou Lin and Julien Mairal and Za{\"{\i}}d
                 Harchaoui",
  title        = "Catalyst Acceleration for First-order Convex
                 Optimization: from Theory to Practice",
  journal      = "J. Mach. Learn. Res.",
  volume       = "18",
  pages        = "212:1--212:54",
  year         = "2017",
}

% accelerated stochastic dual coordinate ascent (SDCA); also based on regularized inner-out loop
@InProceedings{shalev-shwarz2014ASDCA,
  author       = "Shai Shalev{-}Shwartz and Tong Zhang",
  title        = "Accelerated Proximal Stochastic Dual Coordinate Ascent
                 for Regularized Loss Minimization",
  booktitle    = "Proceedings of the 31th International Conference on
                 Machine Learning, {ICML} 2014",
  series       = "{JMLR} Workshop and Conference Proceedings",
  volume       = "32",
  pages        = "64--72",
  year         = "2014",
}

%% primal-dual methods for ind.\ smooth functions

@InProceedings{zhang2015SPDC,
  author       = "Yuchen Zhang and Xiao Lin",
  editor       = "Francis R. Bach and David M. Blei",
  title        = "Stochastic Primal-Dual Coordinate Method for
                 Regularized Empirical Risk Minimization",
  booktitle    = "Proceedings of the 32nd International Conference on
                 Machine Learning, {ICML} 2015",
  series       = "{JMLR} Workshop and Conference Proceedings",
  volume       = "37",
  pages        = "353--361",
  year         = "2015",
}

@Article{lan2018incremental,
  author       = "Guanghui Lan and Yi Zhou",
  title        = "An optimal randomized incremental gradient method",
  journal      = "Math. Program.",
  volume       = "171",
  number       = "1-2",
  pages        = "167--215",
  year         = "2018",
}

@InProceedings{frostig2015unregularizing,
  author       = "Roy Frostig and Rong Ge and Sham M. Kakade and Aaron
                 Sidford",
  editor       = "Francis R. Bach and David M. Blei",
  title        = "Un-regularizing: Approximate proximal point and faster
                 stochastic algorithms for empirical risk minimization",
  booktitle    = "Proceedings of the 32nd International Conference on
                 Machine Learning, {ICML} 2015",
  series       = "{JMLR} Workshop and Conference Proceedings",
  volume       = "37",
  pages        = "2540--2548",
  year         = "2015",
}

@Article{assran2020convergence,
  title        = "On the Convergence of {N}esterov's Accelerated
                 Gradient Method in Stochastic Settings",
  author       = "Mahmoud Assran and Michael Rabbat",
  journal      = "arXiv preprint arXiv:2002.12414",
  year         = "2020",
}

@Article{chen2020understanding,
  title        = "Understanding Accelerated Stochastic Gradient Descent
                 via the Growth Condition",
  author       = "You-Lin Chen and Mladen Kolar",
  journal      = "arXiv preprint arXiv:2006.06782",
  year         = "2020",
}

@InProceedings{jain2018accelerating,
  author       = "Prateek Jain and Sham M. Kakade and Rahul Kidambi and
                 Praneeth Netrapalli and Aaron Sidford",
  editor       = "S{\'{e}}bastien Bubeck and Vianney Perchet and
                 Philippe Rigollet",
  title        = "Accelerating Stochastic Gradient Descent for Least
                 Squares Regression",
  booktitle    = "Conference On Learning Theory, {COLT} 2018",
  series       = "Proceedings of Machine Learning Research",
  volume       = "75",
  pages        = "545--604",
  publisher    = "{PMLR}",
  year         = "2018",
}

# Split Convergence Rates #
@InProceedings{hu2009accelerated,
  author       = "Chonghai Hu and James T. Kwok and Weike Pan",
  editor       = "Yoshua Bengio and Dale Schuurmans and John D. Lafferty
                 and Christopher K. I. Williams and Aron Culotta",
  title        = "Accelerated Gradient Methods for Stochastic
                 Optimization and Online Learning",
  booktitle    = "Advances in Neural Information Processing Systems 22:
                 {NeurIPS} 2009",
  pages        = "781--789",
  publisher    = "Curran Associates, Inc.",
  year         = "2009",
}

@Article{li2015accelerated,
  title        = "Accelerated proximal gradient methods for nonconvex
                 programming",
  author       = "Huan Li and Zhouchen Lin",
  journal      = "Advances in neural information processing systems",
  volume       = "28",
  pages        = "379--387",
  year         = "2015",
}

@Book{bertsekas2014constrained,
  title        = "Constrained optimization and Lagrange multiplier
                 methods",
  author       = "Dimitri P Bertsekas",
  year         = "2014",
  publisher    = "Academic press",
}
